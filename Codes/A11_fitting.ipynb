{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a801294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data_parser\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "    print(\"CUDA avaiable\")\n",
    "else:  \n",
    "    dev = \"cpu\"\n",
    "    print(\"CUDA not avaiable\")\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45898e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_plot_X_y(X_train_label, y_train, train_pred=None, X_val_label=None, y_val=None, \\\n",
    "                  val_pred=None, epoch=None, loss=None, mer=None, output_col=None, title=None, savefig=False, figname=None, unwrap=False):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    F = np.arange(0, 100.2, 0.2)\n",
    "    \n",
    "    if X_val_label is not None:\n",
    "        fig, ax = plt.subplots(2, figsize=(20, 10))\n",
    "        \n",
    "        ax[0].plot(F, y_train.detach().numpy())\n",
    "        if train_pred is not None:\n",
    "            ax[0].plot(F, train_pred.detach().numpy())\n",
    "        ax[0].set_xlabel(X_train_label+' Frequency (GHz)')\n",
    "        if output_col is not None:\n",
    "            ax[0].set_ylabel(output_col)\n",
    "        \n",
    "        ax[1].plot(F, y_val.detach().numpy())\n",
    "        if val_pred is not None:\n",
    "            text_kwargs = dict(fontsize=18, )\n",
    "            ax[1].plot(F, val_pred.detach().numpy())\n",
    "            plt.text(0, 0, 'Batch %d Loss: %.4f Max Error Rate : %.4f' % (epoch, loss, mer), **text_kwargs)\n",
    "    \n",
    "        ax[1].set_xlabel(X_val_label+' Frequency (GHz)')\n",
    "        if output_col is not None:\n",
    "            ax[1].set_ylabel(output_col)\n",
    "        \n",
    "        if title is not None:\n",
    "            ax[1].set_title(title)\n",
    "        \n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, figsize=(20, 10))\n",
    "        ax.plot(F, y_train.detach().numpy())\n",
    "        if train_pred is not None:\n",
    "            text_kwargs = dict(fontsize=18, )\n",
    "            ax.plot(F, pred.detach().numpy())\n",
    "            plt.text(0, 0, 'Batch %d Loss: %.4f Max Error Rate : %.4f' % (epoch, loss, mer), **text_kwargs)  \n",
    "        ax.set_xlabel(X_train_label+' Frequency (GHz)')\n",
    "        if output_col is not None:\n",
    "            ax.set_ylabel(output_col)\n",
    "            \n",
    "        if title is not None:\n",
    "            ax[1].set_title(title)\n",
    "    if unwrap == True:\n",
    "        figname += \"_unwrap\"\n",
    "    if savefig == True:\n",
    "        plt.savefig('../Figures/pdf/'+figname+'.pdf')\n",
    "        plt.savefig('../Figures/png/'+figname+'.png')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_processor():\n",
    "    def __init__(self):\n",
    "        self.dict = {}\n",
    "    \n",
    "    def initial_normalize(self, X, y):\n",
    "        self.dict['X_std'] = X.std(dim=0)\n",
    "        self.dict['X_mean'] = X.mean(dim=0)\n",
    "        self.dict['y_std'] = y.std(dim=0)\n",
    "        self.dict['y_mean'] = y.mean(dim=0)\n",
    "        # new_X = (X - self.dict['X_mean']) / self.dict['X_std']\n",
    "        # new_y = (y - self.dict['y_mean']) / self.dict['y_std']\n",
    "        # return new_X, new_y\n",
    "        return\n",
    "    \n",
    "    def normalize(self, X=None, y=None):\n",
    "        new_X, new_y = None, None\n",
    "        if X is not None:\n",
    "            new_X = (X - self.dict['X_mean']) / self.dict['X_std']\n",
    "        if y is not None:\n",
    "            new_y = (y - self.dict['y_mean']) / self.dict['y_std']\n",
    "        return new_X, new_y\n",
    "        \n",
    "    def denormalize(self, X=None, y=None):\n",
    "        original_X, original_y = None, None\n",
    "        if X is not None:\n",
    "            original_X = X * self.dict['X_std'] + self.dict['X_mean']\n",
    "        if y is not None:    \n",
    "            original_y = y * self.dict['y_std'] + self.dict['y_mean']\n",
    "        return original_X, original_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7676ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class P13_fitting_MLP_6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P13_fitting_MLP_6, self).__init__()\n",
    "        # Define the layers in the model\n",
    "        self.fc1 = nn.Linear(4, 800)\n",
    "        self.fc2 = nn.Linear(800, 800)\n",
    "        self.fc3 = nn.Linear(800, 800)\n",
    "        self.fc4 = nn.Linear(800, 800)\n",
    "        self.fc5 = nn.Linear(800, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagation\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "def initialize_weights(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc2f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, input_cols, output_col, unwrap, result_config):\n",
    "    \n",
    "\n",
    "    \n",
    "    # Define Model name, figure title and figure name\n",
    "    modelpath = '../Models/'+ output_col + '.pt'\n",
    "    figtitle = output_col\n",
    "    figname = output_col\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    index_list = list(dict.fromkeys(df.index.get_level_values(0)))\n",
    "    random.seed(42)\n",
    "    np.random.shuffle(index_list)\n",
    "\n",
    "    # Split the indices into 80% training set, 10% testing set and 10% validation set\n",
    "    train_idx = index_list[:int(len(index_list) * 0.8)]\n",
    "    test_idx = index_list[int(len(index_list) * 0.8):int(len(index_list) * 0.9)]\n",
    "    val_idx = index_list[int(len(index_list) * 0.9):]\n",
    "    X_train, y_train = torch.Tensor(df.loc[train_idx][input_cols].to_numpy()).to(device), torch.Tensor(df.loc[train_idx][output_col].to_numpy().reshape(-1, 1)).to(device)\n",
    "    X_test, y_test = torch.Tensor(df.loc[test_idx][input_cols].to_numpy()).to(device), torch.Tensor(df.loc[test_idx][output_col].to_numpy().reshape(-1, 1)).to(device)\n",
    "    X_val, y_val = torch.Tensor(df.loc[val_idx][input_cols].to_numpy()).to(device), torch.Tensor(df.loc[val_idx][output_col].to_numpy().reshape(-1, 1)).to(device)\n",
    "\n",
    "    # Define an example training, validating index for live plot\n",
    "    train_idx_example = train_idx[0]\n",
    "    val_idx_example = val_idx[0]\n",
    "    X_train_example, y_train_example = torch.Tensor(df.loc[train_idx_example][input_cols].to_numpy()).to(device), torch.Tensor(df.loc[train_idx_example][output_col].to_numpy().reshape(-1, 1)).to(device)\n",
    "    X_val_example, y_val_example = torch.Tensor(df.loc[val_idx_example][input_cols].to_numpy()).to(device), torch.Tensor(df.loc[val_idx_example][output_col].to_numpy().reshape(-1, 1)).to(device)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    dp = data_processor()\n",
    "    X = df[input_cols].to_numpy()\n",
    "    y = df[output_col].to_numpy().reshape(-1, 1)\n",
    "    dp.initial_normalize(torch.Tensor(X).to(device), torch.Tensor(y).to(device))\n",
    "    X_train_norm, y_train_norm = dp.normalize(X_train, y_train)\n",
    "    X_test_norm, y_test_norm = dp.normalize(X_test, y_test)\n",
    "    X_val_norm, y_val_norm = dp.normalize(X_val, y_val)\n",
    "    \n",
    "    # Normalize example data\n",
    "    X_train_example_norm, _ = dp.normalize(X=X_train_example)\n",
    "    X_val_example_norm, _ = dp.normalize(X=X_val_example)\n",
    "    \n",
    "    model = P13_fitting_MLP_6()\n",
    "    learning_rate = 0.01\n",
    "    optimizer = optim.Adamax(model.parameters(), lr=learning_rate)\n",
    "    model = model.to(device)\n",
    "\n",
    "    epoch = 0\n",
    "    num_epochs = 30000\n",
    "    \n",
    "    # Define the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    #for epoch in range(num_epochs)\n",
    "    train_loss = 1.0\n",
    "    best_val_mse = 100.0\n",
    "    best_val_mer = -100.0\n",
    "    \n",
    "    train_size = X_train.shape[0]\n",
    "    batch_size = 501 * 1\n",
    "    batch_start = 0\n",
    "    \n",
    "    mini_epoch = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        batch_end = batch_start + batch_size\n",
    "        if batch_end >= train_size:\n",
    "            X_train_batch = torch.cat((X_train_norm[batch_start:train_size], X_train_norm[0:(batch_end-train_size)]))\n",
    "            y_train_batch = torch.cat((y_train_norm[batch_start:train_size], y_train_norm[0:(batch_end-train_size)]))\n",
    "            batch_start = batch_end - train_size\n",
    "        else:\n",
    "            X_train_batch = X_train_norm[batch_start:batch_end]\n",
    "            y_train_batch = y_train_norm[batch_start:batch_end]\n",
    "            batch_start = batch_end\n",
    "        \n",
    "        for i in range(mini_epoch):\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward propagation\n",
    "            outputs = model(X_train_batch)\n",
    "\n",
    "            # Calculate the loss function\n",
    "            loss = criterion(outputs, y_train_batch)\n",
    "\n",
    "            # Backward propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate original values\n",
    "        _, orignial_outputs = dp.denormalize(y=outputs)\n",
    "        _, original_y_train = dp.denormalize(y=y_train_batch)\n",
    "        \n",
    "        # Validation\n",
    "        if epoch % 100 == 99: \n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Make predictions on the test data\n",
    "                predictions = model(X_val_norm)\n",
    "\n",
    "                # Calculate the loss function\n",
    "                val_loss = criterion(predictions, y_val_norm)\n",
    "\n",
    "                # Track the loss value \n",
    "                val_loss = val_loss.item()\n",
    "\n",
    "                _, orignial_predictions = dp.denormalize(y=predictions)\n",
    "                _, original_y_val = dp.denormalize(y=y_val_norm)\n",
    "                \n",
    "                # Compute the max error rate\n",
    "                mer = torch.max(torch.abs(orignial_predictions - original_y_val) / original_y_val)\n",
    "                \n",
    "                # Save best model\n",
    "                if val_loss < best_val_mse:\n",
    "                    best_val_mse = val_loss\n",
    "                    best_val_mer = mer\n",
    "                    torch.save(model.state_dict(), modelpath) \n",
    "                    \n",
    "                # Example data\n",
    "                predictions = model(X_train_example_norm)\n",
    "                _, orignial_train_predictions = dp.denormalize(y=predictions)\n",
    "                \n",
    "                predictions = model(X_val_example_norm)\n",
    "                _, original_val_predictions = dp.denormalize(y=predictions)\n",
    "                \n",
    "\n",
    "                live_plot_X_y(train_idx_example, y_train_example.cpu(), orignial_train_predictions.cpu(), val_idx_example, \\\n",
    "                              y_val_example.cpu(), original_val_predictions.cpu(), epoch + 1, val_loss, \\\n",
    "                              mer, output_col=output_col, title=figtitle, savefig=((epoch + 1) == num_epochs), figname=figname, unwrap)\n",
    "    # Testing\n",
    "    \n",
    "    model.load_state_dict(torch.load(modelpath))\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        predictions = model(X_test_norm)\n",
    "\n",
    "        # Calculate the loss function\n",
    "        test_loss = criterion(predictions, y_test_norm)\n",
    "\n",
    "        # Track the loss value \n",
    "        test_loss = test_loss.item()  \n",
    "        \n",
    "        _, orignial_predictions = dp.denormalize(y=predictions)\n",
    "        _, original_y_test = dp.denormalize(y=y_test_norm)\n",
    "        \n",
    "        # Compute the max error rate\n",
    "        mer = torch.max(torch.abs(orignial_predictions - original_y_test) / original_y_test)\n",
    "\n",
    "    # Save results\n",
    "    result_lst = [[output_col, best_val_mse, best_val_mer, test_loss, mer]]\n",
    "    result_config['df'] = pd.concat([result_config['df'], pd.DataFrame(result_lst, columns=result_config['columns'])], ignore_index=True)\n",
    "    return result_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b87ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_parser.calculate_amp_phase(data_parser.data_parse(), unwrap=False)\n",
    "df_unwrap = data_parser.calculate_amp_phase(data_parser.data_parse()) # unwrap the phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "153f6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.to_numpy()\n",
    "df_unwrap.index.to_numpy()\n",
    "input_cols = [\"F\", \"W\", \"Trap\", \"Length\"]\n",
    "output_col = 'P(1,3)'\n",
    "\n",
    "columns = ['S_parameter', 'best_val_mse', 'best_val_mer', 'test_mse', 'test_mer']\n",
    "result_config = {}\n",
    "result_config['df'] = pd.DataFrame(columns=columns)\n",
    "result_config['columns'] = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41a1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "result_config = train(df, input_cols, output_col, result_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e51755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
